{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ft-drift\n",
    "> Check for data drift by comparing two OpenAI chat jsonl files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ft-drift` helps you check for data drift by comparing two OpenAI [multi-turn chat jsonl files](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install ft_drift\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Checking for dataset drift can help you debug if:\n",
    "\n",
    "1. Your model is trained on data that doesn't reflect production (different prompts, functions, etc).\n",
    "2. Your training data contains unexpected or accidental artifacts.\n",
    "\n",
    "In either situation, you can compare data from relevant sources (i.e. production vs fine-tuning) to find unwanted changes.  This is one of the most common source of errors when fine-tuning models!  \n",
    "\n",
    "The demo below shows a cli tool used to detect data drift between two files, `file_a.jsonl` and `file_b.jsonl`.  Afterwards, a table of important tokens that account for the drift are shown, such as:\n",
    "\n",
    "- `END-UI-FORMAT`\n",
    "- `UI-FORMAT`\n",
    "- \"```json\"\n",
    "- etc.\n",
    "\n",
    "## Usage\n",
    "\n",
    "After installing `ft_drift`, the cli command `detect_drift` will be available to you.  **Currently, `ft_drift` only detects drift in prompt templates and other token-based drift (as opposed to symantic drift)**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](drift_cli.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does it Work?\n",
    "\n",
    "This works by doing the following steps:\n",
    "\n",
    "1. Fit a binary classifier (random forest) to discriminate between two datasets.\n",
    "2. If the classifier can predict a material difference (ex: AUC >= 0.60) then we know there is drift (something is systematically different b/w the two datasets).\n",
    "3. We show the most important features from the classifier which are tokens (segments of text) to help you debug what is different. \n",
    "\n",
    "If this tool doesn't detect drift, it doesn't mean drift doesn't exist.  It just means we didn't find it.  For more background on this approach, see this slide from [my talk on MLOps tools](https://www.youtube.com/watch?v=GHk5HMW4XMA):\n",
    "\n",
    "![](drift_tfx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Other things that could be added:\n",
    "\n",
    "- [ ] Semantic drift by incorporating embeddings.\n",
    "- [ ] More features: length of messages, # of turns etc.\n",
    "- [ ] Wiring up the function definition diff to the CLI (I don't need this yet for my use case).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
